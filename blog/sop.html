<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Emergent (Mis)Alignment: Code Security and AI Alignment</title>
    <link rel="stylesheet" href="../css/emergent.css">
    <!-- <link href="https://fonts.googleapis.com/css2?family=Courier+Prime:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet"> -->
</head>
<body>
    <div class="button-row">
        <button id="contrast" class="cont-inv" onclick="toggleContrast()">contrast</button>
        <button id="invmode" class="cont-inv" onclick="toggleInvert()">invert</button>
    </div>

    <article>
        <h1> Statement of purpose </h1>
        <h2 style="align-items: left;">What?</h2>
        <p>
            Besides the general definition of a statement of purpose (see: <a href="https://users.ece.cmu.edu/~mabdelm/statement-of-purpose-tips.html">this</a>) an SoP involves
            <ul>
                <li>what you did</li>
                <li>what did you learn from it.</li>
                <li>what do you want to do!</li>
            </ul>
            A major part of the SoP is tying your past experiences with your future goals. 
            In computer science, SoPs usually involve you being a lot more direct about the points that I've stated above. This may not apply to other domains though. 
        </p>
        <h2 style="align-items: left;">My annotated statement of purpose</h2>
        <p>
            Below is my annotated statement of purpose for graduate school applications. I've added comments to explain the reasoning behind each section.
            I would also encourage you to <a href="https://saujasv.github.io/annotated-sop">check out Saujas' annotated statement</a>. 
            Also, a note: I tend to use prescriptive language in my blogposts. This comes purely from <a href="https://web.mit.edu/curhan/www/docs/Articles/15341_Readings/Influence_Compliance/Markus_Kitayama_1991_Culture_and_the_self.pdf"> my own conditioning </a>. However, none of these annotations are prescriptive how-to guides on writing SoPs; they're simply to provide my internal thought process of why I chose to do what I did, with the intent that it may help inform you in your own writing. 
        </p>
        <blockquote>
            I aim to pursue a Ph.D. at the intersection of Natural Language Processing (NLP), Safety, and Security. I am keen on going beyond chat-based interactions and exploring systems that are granted forms of agency. This interest stems from my realization that generative harms are
            not limited to chat interfaces or user-system interactions alone; when models can take actions, the potential for unintended consequences increases significantly. Below, I highlight my experiences surrounding AI safety, security and ethics that have shaped my research interests and led me to consider pursuing this problem.
        </blockquote>
        The aim of the first paragraph of my Statement was to clearly state my research topic, and the subproblems in which I'm looking to research on. I segue into the actual motivation behind considering this topic through my final lines ("I highlight my experiences")

        <blockquote>
            <b>Jailbreaking and LLM Safety:</b> As large language models get increasingly powerful in terms of their capabilities, they are also increasingly prone to causing harm to their users.
            This is something I quickly realized as a Research Fellow at Microsoft Turing - around late 2022, I discovered early on in a bug bash for our product, Bing Chat, that it exhibited surprisingly toxic generations when cleverly prompted to do so. While such prompts are
            established as jailbreaks, there was little analysis at the time that understood the extent and applicability of these harms. Hence, I initiated a project studying jailbreaks with my
            mentors, Prof. <b>Monojit Choudhury</b> and Prof. <b>Somak Aditya</b>, curating an ontology of jailbreak types, lexical categories, and their attacker’s intentions. Through this work, I
            uncovered that jailbreaks were further exacerbated when different alignment techniques were applied and upon scaling model parameters 
            To better capture attack success I further proposed a two-subject evaluation paradigm: whether model outputs are misaligned from
            the original task, and if attacker intents are satisfied. This work was published at LREC-CoLING 2024[1]. 
        </blockquote>
        Unlike what a lot of people do in their SoPs, I decided to use section headers to clearly demarcate my different research experiences. 
        This is rather common in computer science SoPs, especially in empirical AI research, where people tend to work on several not-so-very chronologically connected research projects.
        One key thing to note here is that you do NOT need to include every single research experience you've had. 
        Furthermore, you do not need to explain your life's story and how you transformed over the course of a few years. The admissions committee looks for:
        <ul>
            <li>What research problems have you worked on and how did they inform your goals?</li>
            <li>What was your role in the project and your key contributions?</li>
            <li>What did you learn from the project?</li>
        </ul>
        I try to answer these questions in each of my research experience sections. 
        <blockquote>    
            This was a great learning experience for me; I learned how to formalize an unfamiliar and understudied problem by defining a jailbreak 
            ontology, curating a dataset and evaluating models on multiple axes.
        </blockquote>
        When I mentioned that I specified what I learned from projects, I don't necessarily mean the technical learnings from my results alone! That would be simply a summary of the paper, or a rehash of my CV.
        What's more important is <a href="https://harshitadd.netlify.app/advice">deduplication</a>. My personal learnings and reflections cannot be specified in a paper or CV, so I decided to include them over here. 
        <blockquote>
            Realizing that most jailbreak methods focused solely on chat-based systems led me to explore broader threat models - I formed a team and secured a $250,000 grant to work on the Amazon Trusted AI Challenge - an attack-defense competition to develop and red-team
            Code LLMs. Advised by Prof. <b>Carolyn Rose</b> from the Language technologies institute (LTI) and Prof. <b>Michael Hilton</b> from Software and Societal System Department (S3D), CMU, I’m
            working on minimizing code-vulnerabilities, such as those from the Common Weakness Enumeration (CWEs) by leveraging code-vulnerability datasets such as the MITRE ATT&CK Python CWEs to develop strong attack vectors in a black-boxed setup.
        </blockquote>
        A sidenote, while I was very authoritative in this paragraph, I ended up not being the lead of this project for reasons that I will not get into here. I made sure to clarify this explicitly with my interviewers. However, this paragraph is what caught the eye of most of my interviewers, as it mentioned two things - 
        (1) I was able to secure funding for a project, and (2) I was able to mobilize a team to work on a problem I cared about. Both of these are SUPER important skills for an academic researcher, as you will invariably need to write grants and lead teams in your future career.
        <blockquote>
        <b>Future directions</b>: During my graduate studies, I wish to study threat models that aren’t restricted to toxic/harmful generation. While reviewing existing work for the Amazon Challenge, I noticed that singular defenses like circuit breakers [4] may not handle insecure
        code generation. Hence, it is necessary to have comprehensive security frameworks that can anticipate and mitigate unintended harmful actions by AI systems. Additionally, developing robust monitoring and evaluation systems [5] when multiple actors/agents are involved can add multiple layers of complexity to the problem.
        </blockquote>
        There are many ways to write about your future directions. I personally preferred to structure my future directions in a separate paragraph at the end of each of my research experience sections, as they were fairly disconnected and non-chronological. 
        My research directions are rather generic - I don't even specify concrete examples of the "threat models" I was mentioning! However, they're specific enough to show that I have a clear idea of what I want to research on. This kind of balance is rather crucial in an SoP; 
        a super generic direction (e.g. I want to work on AI alignment) is not very convincing and doesn't give you an edge, while a super specific direction (e.g. I want to study the effect of insecure code generation on industrial package development) may box you in to a very limited scope, pushing you away from potential advisors.
        A general tip here is to optimize for recall when in doubt - though I wanted to work on AI Security, I applied to professors who were studying LLMs' dual use from the angle of social biases to as far as software development.
        <blockquote>
        <b>AI-Ethics; cultural representation harms, and generalizability</b>: AI systems have been known to misrepresent certain sections of society by directly undermining or failing to recognize their core values: I studied the detrimental nature of alignment methodologies on
        LLM’s ethical reasoning abilities in a position paper I led on value pluralism. I designed a framework, developing a systematic evaluation paradigm to evaluate the ethical reasoning
        capabilities of a language model, encompassing moral dilemmas over multiple granularities (EMNLP Findings, WiNLP Keynote 2023 [2]). This work advocated that LLMs should be
        value-neutral, and that value-alignment should occur at the application level. This project was my first introduction to interdisciplinary research, and it required that I studied moral
        frameworks surrounding the ethical triple theory, and to effectively communicate their subjective ideas amongst team members. 
        The results from this effort proved particularly interesting: LLMs hinted a western centric bias in their reasoning, and they were unable to
        adapt their outputs to user-provided policies. I wished to translate this philosophical position into a practically usable resource to measure this adaptability further.
        </blockquote>
        In hindsight, my personal learnings are rather underwhelming here. However, I really did not wish to explore this direction during my PhD, and this parargraph served more as a segue into my next research experience.
    <blockquote>
When I entered CMU, I noticed work surrounding NLP and cultural studies borrowed frameworks that directly evaluated humans’ intrinsic moral values. However, an LLM
deployed worldwide should largely value neutral and should also be able to adapt to users from different backgrounds. Hence, in a project supervised by Prof. <b>Maarten Sap</b>, I designed
a dataset, NormAd-Eti [3], to evaluate cultural adaptability of language models by contextualizing social situations, with coarse grained geographical information, and finer
grained social norms. Through this framework, we encountered several key insights (1) language models indeed have a western-centric bias when contextualized with social norms,
(2) this bias is amplified through value-alignment strategies. (C3NLP workshop @ ACL 2024; in review at NAACL 2025).
    </blockquote>
        This paragraph's main intent was to show that I was able to take my research projects forward and utilize my resources in two completely different environments to still work on a similar problem. 
    <blockquote> 
        <b>Future Directions</b>: The findings uncovered by NormAd makes me more curious about other biases related to non-chat setups that are exacerbated during the value alignment
stage. Furthermore, I believe multi-agent setups can offer a different perspective on biases which chat based setups may not cover. I am interested in conducting simulation studies [6]
to explore how phenomena surrounding bias-propagation such as information cascades may show up in multi-agent systems and their resultant effects on decision-making.
    </blockquote>
    While my research experiences on biases and ethics were not directly related to my main research interest surrounding AI security, I was still able to tie them in through my future directions; the goal here was recall optimization again. 
    <blockquote>
    <b>Vision for graduate school</b>: Pursuing a Ph.D. at <b>X</b> will equip me to
address these challenges and contribute meaningfully to AI safety and security, and prepare
me for a research-focussed career. I'm interested to work with <b>Y</b>; her focus
on <b>ABC</b> aligns strongly with my current interests around the problem. Given my background in AI ethics and safety, I believe I am
well-positioned to examine both representational and generational harms that AI systems
can produce. To this end, Prof. <b>Z's</b> research vision of <b>DEF</b> aligns deeply with my research goals during my
graduate studies. Notably, her projects on <b>GHI</b> overlap with a broad set of problems I wish to pursue. Overall, <b>X</b> is a great
place for me to pursue my research aspirations, as I can learn from its diverse student group and esteemed faculty.
    </blockquote>
    This final paragraph is rather standard in most SoPs - you need to specify why you're applying to this specific institution, why it's a good fit, and who you wish to work with. 
    I made sure to clearly specify how my research interests aligned with the professors I was applying to work with, and how their research aligned with my goals.
    Note that I was intentionally vague about what I wanted to do with my PhD - this was in part because I wasn't too sure of it myself. But I was leaning towards heading back to the industry. 
    I would recommend being more open and specific about this; as all of my interviewers (I had 10!) asked me about my future career plans.

    </pre>  
    </article>
</body>
    <script>
        function toggleContrast() {
            document.documentElement.classList.toggle('contrast');
        }

        function toggleInvert() {
            document.documentElement.classList.toggle('inverted');
        }
    </script>
</html>